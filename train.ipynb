{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BApA_WT2rf1L","executionInfo":{"status":"ok","timestamp":1602939468456,"user_tz":-480,"elapsed":1018,"user":{"displayName":"Ding Hao Tan","photoUrl":"","userId":"02102124816910855943"}},"outputId":"39aa8d59-b99a-4c7c-b2f4-261d4fe7bcce","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["'''Run this if using Google Colab'''\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KF58AL3Ri8Zg","executionInfo":{"status":"ok","timestamp":1602939470667,"user_tz":-480,"elapsed":3209,"user":{"displayName":"Ding Hao Tan","photoUrl":"","userId":"02102124816910855943"}},"outputId":"44f49c95-bdca-4f0b-d40f-8353895b1ac9","colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["%cd /content/drive/My Drive/MLDA_Hackathon/Codes\n","!pip install import-ipynb\n","import import_ipynb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MLDA_Hackathon/Codes\n","Requirement already satisfied: import-ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eFrFZgMBr3QT"},"source":["from __future__ import print_function, division\n","\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.backends.cudnn as cudnn\n","import matplotlib\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","#from PIL import Image\n","import time\n","import os\n","import model\n","# import model\n","from random_erasing import RandomErasing\n","import yaml\n","import math\n","from shutil import copyfile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FfoXFtyp05P"},"source":["**Checking GPU usage**"]},{"cell_type":"code","metadata":{"id":"jhZQRMXbpgA7","executionInfo":{"status":"ok","timestamp":1602939476783,"user_tz":-480,"elapsed":9307,"user":{"displayName":"Ding Hao Tan","photoUrl":"","userId":"02102124816910855943"}},"outputId":"9c944a50-be3b-42bc-a6e2-969eb8fd59e6","colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","\n","# Import packages\n","import os,sys,humanize,psutil,GPUtil\n","\n","# Define function\n","def mem_report():\n","  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n","  \n","  GPUs = GPUtil.getGPUs()\n","  for i, gpu in enumerate(GPUs):\n","    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n","    \n","# Execute function\n","mem_report()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","CPU RAM Free: 9.8 GB\n","GPU 0 ... Mem Free: 11928MB / 15079MB | Utilization  21%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qLOK6vyYp6BU"},"source":["Let's Start!\n"]},{"cell_type":"code","metadata":{"id":"cZMljHTssVoO","executionInfo":{"status":"ok","timestamp":1602939476787,"user_tz":-480,"elapsed":9305,"user":{"displayName":"Ding Hao Tan","photoUrl":"","userId":"02102124816910855943"}},"outputId":"32bb8603-84e1-41a2-97c9-ee3b6ebe2305","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["version =  torch.__version__\n","#fp16\n","try:\n","    from apex.fp16_utils import *\n","    from apex import amp, optimizers\n","except ImportError: # will be 3.x series\n","    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sP6FD1stsYmd"},"source":["parser = argparse.ArgumentParser(description='Training')\n","parser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\n","parser.add_argument('--name',default='ft_ResNet50', type=str, help='output model name')\n","parser.add_argument('--data_dir',default='/content/drive/My Drive/MLDA_Hackathon/Codes/Data Set/Market/pytorch',type=str, help='training dir path')\n","parser.add_argument('--train_all', action='store_true', help='use all training data' )\n","parser.add_argument('--color_jitter', action='store_true', help='use color jitter in training' )\n","parser.add_argument('--batchsize', default=32, type=int, help='batchsize')\n","parser.add_argument('--stride', default=2, type=int, help='stride')\n","parser.add_argument('--erasing_p', default=0, type=float, help='Random Erasing probability, in [0,1]')\n","parser.add_argument('--use_d  e', action='store_true', help='use densenet121' )\n","parser.add_argument('--use_NAS', action='store_true', help='use NAS' )\n","parser.add_argument('--warm_epoch', default=0, type=int, help='the first K epoch that needs warm up')\n","parser.add_argument('--lr', default=0.05, type=float, help='learning rate')\n","parser.add_argument('--droprate', default=0.5, type=float, help='drop rate')\n","parser.add_argument('--PCB', action='store_true', help='use PCB+ResNet50' )\n","parser.add_argument('--fp16', action='store_true', help='use float16 instead of float32, which will save about 50% memory' )\n","opt = parser.parse_args(args=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXPZpu5vscy2"},"source":["fp16 = opt.fp16\n","data_dir = opt.data_dir\n","name = opt.name\n","str_ids = opt.gpu_ids.split(',')\n","gpu_ids = []\n","for str_id in str_ids:\n","    gid = int(str_id)\n","    if gid >=0:\n","        gpu_ids.append(gid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuKH15HBsiEl"},"source":["# set gpu ids\n","if len(gpu_ids)>0:\n","    torch.cuda.set_device(gpu_ids[0])\n","    cudnn.benchmark = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yg_UOsW_s48Q"},"source":["Load Data"]},{"cell_type":"code","metadata":{"id":"B88iXw4SsuY_","executionInfo":{"status":"ok","timestamp":1602939479129,"user_tz":-480,"elapsed":11625,"user":{"displayName":"Ding Hao Tan","photoUrl":"","userId":"02102124816910855943"}},"outputId":"a7e32b31-21e2-4535-b481-0c2713a0fb14","colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["transform_train_list = [\n","        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n","        transforms.Resize((256,128), interpolation=3),\n","        transforms.Pad(10),\n","        transforms.RandomCrop((256,128)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]\n","\n","transform_val_list = [\n","        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]\n","\n","if opt.PCB:\n","    transform_train_list = [\n","        transforms.Resize((384,192), interpolation=3),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]\n","    transform_val_list = [\n","        transforms.Resize(size=(384,192),interpolation=3), #Image.BICUBIC\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]\n","\n","if opt.erasing_p>0:\n","    transform_train_list = transform_train_list +  [RandomErasing(probability = opt.erasing_p, mean=[0.0, 0.0, 0.0])]\n","\n","if opt.color_jitter:\n","    transform_train_list = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0)] + transform_train_list\n","\n","print(transform_train_list)\n","data_transforms = {\n","    'train': transforms.Compose( transform_train_list ),\n","    'val': transforms.Compose(transform_val_list),\n","}\n","\n","\n","train_all = ''\n","if opt.train_all:\n","     train_all = '_all'\n","\n","image_datasets = {}\n","image_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n","                                          data_transforms['train'])\n","image_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n","                                          data_transforms['val'])\n","\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n","                                             shuffle=True, num_workers=8, pin_memory=True) # 8 workers may work faster\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","\n","use_gpu = torch.cuda.is_available()\n","\n","since = time.time()\n","inputs, classes = next(iter(dataloaders['train']))\n","print(time.time()-since)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=None), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n","1.1175217628479004\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u0Y8i7kJtKrw"},"source":["# **Training the model**\n"]},{"cell_type":"code","metadata":{"id":"XJQUcC4ltOOO"},"source":["y_loss = {} # loss history\n","y_loss['train'] = []\n","y_loss['val'] = []\n","y_err = {}\n","y_err['train'] = []\n","y_err['val'] = []\n","\n","def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    #best_model_wts = model.state_dict()\n","    #best_acc = 0.0\n","    warm_up = 0.1 # We start from the 0.1*lrRate\n","    warm_iteration = round(dataset_sizes['train']/opt.batchsize)*opt.warm_epoch # first 5 epoch\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","        \n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                scheduler.step()\n","                model.train(True)  # Set model to training mode\n","            else:\n","                model.train(False)  # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","            # Iterate over data.\n","            for data in dataloaders[phase]:\n","                # get the inputs\n","                inputs, labels = data\n","                now_batch_size,c,h,w = inputs.shape\n","                if now_batch_size<opt.batchsize: # skip the last batch\n","                    continue\n","                #print(inputs.shape)\n","                # wrap them in Variable\n","                if use_gpu:\n","                    inputs = Variable(inputs.cuda().detach())\n","                    labels = Variable(labels.cuda().detach())\n","                else:\n","                    inputs, labels = Variable(inputs), Variable(labels)\n","                # if we use low precision, input also need to be fp16\n","                #if fp16:\n","                #    inputs = inputs.half()\n"," \n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                if phase == 'val':\n","                    with torch.no_grad():\n","                        outputs = model(inputs)\n","                else:\n","                    outputs = model(inputs)\n","\n","                if not opt.PCB:\n","                    _, preds = torch.max(outputs.data, 1)\n","                    loss = criterion(outputs, labels)\n","                else:\n","                    part = {}\n","                    sm = nn.Softmax(dim=1)\n","                    num_part = 6\n","                    for i in range(num_part):\n","                        part[i] = outputs[i]\n","\n","                    score = sm(part[0]) + sm(part[1]) +sm(part[2]) + sm(part[3]) +sm(part[4]) +sm(part[5])\n","                    _, preds = torch.max(score.data, 1)\n","\n","                    loss = criterion(part[0], labels)\n","                    for i in range(num_part-1):\n","                        loss += criterion(part[i+1], labels)\n","\n","                # backward + optimize only if in training phase\n","                if epoch<opt.warm_epoch and phase == 'train': \n","                    warm_up = min(1.0, warm_up + 0.9 / warm_iteration)\n","                    loss *= warm_up\n","\n","                if phase == 'train':\n","                    if fp16: # we use optimier to backward loss\n","                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                            scaled_loss.backward()\n","                    else:\n","                        loss.backward()\n","                    optimizer.step()\n","\n","                # statistics\n","                if int(version[0])>0 or int(version[2]) > 3: # for the new version like 0.4.0, 0.5.0 and 1.0.0\n","                    running_loss += loss.item() * now_batch_size\n","                else :  # for the old version like 0.3.0 and 0.3.1\n","                    running_loss += loss.data[0] * now_batch_size\n","                running_corrects += float(torch.sum(preds == labels.data))\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects / dataset_sizes[phase]\n","            \n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","            \n","            y_loss[phase].append(epoch_loss)\n","            y_err[phase].append(1.0-epoch_acc)            \n","            # deep copy the model\n","            if phase == 'val':\n","                last_model_wts = model.state_dict()\n","                if epoch%10 == 9:\n","                    save_network(model, epoch)\n","                draw_curve(epoch)\n","\n","        time_elapsed = time.time() - since\n","        print('Training complete in {:.0f}m {:.0f}s'.format(\n","            time_elapsed // 60, time_elapsed % 60))\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    #print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(last_model_wts)\n","    save_network(model, 'last')\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ysRKsHuEttzg"},"source":["Drawing the model"]},{"cell_type":"code","metadata":{"id":"aMFkwMZXtqQU"},"source":["x_epoch = []\n","fig = plt.figure()\n","ax0 = fig.add_subplot(121, title=\"loss\")\n","ax1 = fig.add_subplot(122, title=\"top1err\")\n","def draw_curve(current_epoch):\n","    x_epoch.append(current_epoch)\n","    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n","    ax0.plot(x_epoch, y_loss['val'], 'ro-', label='val')\n","    ax1.plot(x_epoch, y_err['train'], 'bo-', label='train')\n","    ax1.plot(x_epoch, y_err['val'], 'ro-', label='val')\n","    if current_epoch == 0:\n","        ax0.legend()\n","        ax1.legend()\n","    fig.savefig( os.path.join('./trained_model',name,'train.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZeQXeqbOt2kn"},"source":["Saving the model"]},{"cell_type":"code","metadata":{"id":"V4IRLk5_tzjE"},"source":["def save_network(network, epoch_label):\n","    save_filename = 'net_%s.pth'% epoch_label\n","    save_path = os.path.join('./trained_model',name,save_filename)\n","    torch.save(network.cpu().state_dict(), save_path)\n","    if torch.cuda.is_available():\n","        network.cuda(gpu_ids[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SnpYJRq5uCso"},"source":["Finetuning the convnet\n","\n","Load a pretrainied model and reset final fully connected layer.\n"]},{"cell_type":"code","metadata":{"id":"d4NzXCLMt7SQ","executionInfo":{"status":"ok","timestamp":1602939480114,"user_tz":-480,"elapsed":12593,"user":{"displayName":"Ding Hao Tan","photoUrl":"","userId":"02102124816910855943"}},"outputId":"acbdbb4a-90ac-402f-9937-cb2a608c1872","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if opt.use_dense:\n","    model = ft_net_dense(len(class_names), opt.droprate)\n","elif opt.use_NAS:\n","    model = ft_net_NAS(len(class_names), opt.droprate)\n","else:\n","    model = ft_net(len(class_names), opt.droprate, opt.stride)\n","\n","if opt.PCB:\n","    model = PCB(len(class_names))\n","\n","opt.nclasses = len(class_names)\n","\n","print(model)\n","\n","if not opt.PCB:\n","    ignored_params = list(map(id, model.classifier.parameters() ))\n","    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n","    optimizer_ft = optim.SGD([\n","             {'params': base_params, 'lr': 0.1*opt.lr},\n","             {'params': model.classifier.parameters(), 'lr': opt.lr}\n","         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n","else:\n","    ignored_params = list(map(id, model.model.fc.parameters() ))\n","    ignored_params += (list(map(id, model.classifier0.parameters() )) \n","                     +list(map(id, model.classifier1.parameters() ))\n","                     +list(map(id, model.classifier2.parameters() ))\n","                     +list(map(id, model.classifier3.parameters() ))\n","                     +list(map(id, model.classifier4.parameters() ))\n","                     +list(map(id, model.classifier5.parameters() ))\n","                     #+list(map(id, model.classifier6.parameters() ))\n","                     #+list(map(id, model.classifier7.parameters() ))\n","                      )\n","    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n","    optimizer_ft = optim.SGD([\n","             {'params': base_params, 'lr': 0.1*opt.lr},\n","             {'params': model.model.fc.parameters(), 'lr': opt.lr},\n","             {'params': model.classifier0.parameters(), 'lr': opt.lr},\n","             {'params': model.classifier1.parameters(), 'lr': opt.lr},\n","             {'params': model.classifier2.parameters(), 'lr': opt.lr},\n","             {'params': model.classifier3.parameters(), 'lr': opt.lr},\n","             {'params': model.classifier4.parameters(), 'lr': opt.lr},\n","             {'params': model.classifier5.parameters(), 'lr': opt.lr},\n","             #{'params': model.classifier6.parameters(), 'lr': 0.01},\n","             #{'params': model.classifier7.parameters(), 'lr': 0.01}\n","         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n","\n","# Decay LR by a factor of 0.1 every 40 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=40, gamma=0.1)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ft_net(\n","  (model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n","  )\n","  (classifier): ClassBlock(\n","    (add_block): Sequential(\n","      (0): Linear(in_features=2048, out_features=512, bias=True)\n","      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): Dropout(p=0.5, inplace=False)\n","    )\n","    (classifier): Sequential(\n","      (0): Linear(in_features=512, out_features=751, bias=True)\n","    )\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p-Cpy0UWuSNI"},"source":["Train and evaluate\n","\n","It should take around 1-2 hours on GPU. \n"]},{"cell_type":"code","metadata":{"id":"7Y-4g19nuWTJ","outputId":"2e20dd99-5a1d-41f4-d46e-90298c78846f","colab":{"base_uri":"https://localhost:8080/","height":546}},"source":["dir_name = os.path.join('/content/drive/My Drive/MLDA_Hackathon/Codes/Data Set/Market/pytorch/',name)\n","if not os.path.isdir(dir_name):\n","    os.mkdir(dir_name)\n","#record every run\n","copyfile('./train.ipynb', dir_name+'/train.ipynb')                      #remember to convert file type outside of colab\n","copyfile('./model.ipynb', dir_name+'/model.ipynb')                      #remember to convert file type outside of colab                \n","\n","# save opts\n","with open('%s/opts.yaml'%dir_name,'w') as fp:\n","    yaml.dump(vars(opt), fp, default_flow_style=False)\n","\n","# model to gpu\n","model = model.cuda()\n","if fp16:\n","    #model = network_to_half(model)\n","    #optimizer_ft = FP16_Optimizer(optimizer_ft, static_loss_scale = 128.0)\n","    model, optimizer_ft = amp.initialize(model, optimizer_ft, opt_level = \"O1\")\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=60)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0/59\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 3.8361 Acc: 0.2844\n","val Loss: 1.7933 Acc: 0.5539\n","Training complete in 1m 19s\n","\n","Epoch 1/59\n","----------\n","train Loss: 1.2172 Acc: 0.6849\n","val Loss: 0.7363 Acc: 0.7736\n","Training complete in 2m 39s\n","\n","Epoch 2/59\n","----------\n","train Loss: 0.6658 Acc: 0.8166\n","val Loss: 0.4622 Acc: 0.8535\n","Training complete in 4m 3s\n","\n","Epoch 3/59\n","----------\n","train Loss: 0.4427 Acc: 0.8789\n","val Loss: 0.2958 Acc: 0.9041\n","Training complete in 5m 26s\n","\n","Epoch 4/59\n","----------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RJwoiBHs29xH"},"source":[""],"execution_count":null,"outputs":[]}]}